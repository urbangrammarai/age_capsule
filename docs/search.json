[
  {
    "objectID": "gpu_spatial_join.html",
    "href": "gpu_spatial_join.html",
    "title": "Age Capsule",
    "section": "",
    "text": "We run this on the latest official RAPIDS container (on a NVIDIA GPU accelerated machine), which we can launch with:\ndocker run --gpus all --rm -it \\\n    -p 8889:8888 -p 8788:8787 -p 8786:8786 \\\n    -v /media/dani/DataStore/data/:/rapids/notebooks/data \\\n    -v ${PWD}:/rapids/notebooks/work \\\n    rapidsai/rapidsai:cuda11.4-runtime-ubuntu20.04-py3.9\nWith this setup, we can access the same work and data folders as in the previous notebook.\n\nimport geopandas\nimport cuspatial\nimport pandas\nfrom tools import sjoin_gpu\nfrom tqdm import tqdm\nfrom math import ceil\n\nuprn_p = '/rapids/notebooks/data/tmp/epc_uprn.pq'\nss_p = '/rapids/notebooks/data/tmp/sss.pq'\npc_p = '/rapids/notebooks/data/tmp/postcode_pts.pq'\n\n\n\nBefore we run the spatial join on the whole dataset, and since cuspatial is a relatively new library compared to geopandas, we perform a check on a small sample to confirm the results from the spatial join are the same.\nWe will read into RAM the first 1,600 EPC properties (uprn) and joined them to the spatial signature polygons (ss):\n\n%%time\nuprn = geopandas.read_parquet(uprn_p).head(1600)\nss = geopandas.read_parquet(ss_p)\n\nCPU times: user 26.2 s, sys: 7.71 s, total: 34 s\nWall time: 29.5 s\n\n\nThen we move them to the GPU:\n\n%%time\nuprn_gpu = cuspatial.from_geopandas(uprn)\nss_gpu = cuspatial.from_geopandas(ss)\n\nCPU times: user 7.78 s, sys: 675 ms, total: 8.45 s\nWall time: 8.39 s\n\n\nAnd perform the GPU-backed spatial join:\n\n%time tst_gpu = sjoin_gpu(uprn_gpu, ss_gpu)\n\n/opt/conda/envs/rapids/lib/python3.9/site-packages/cuspatial/core/spatial/indexing.py:193: UserWarning: scale 5 is less than required minimum scale 9345.561538461538. Clamping to minimum scale\n  warnings.warn(\n/opt/conda/envs/rapids/lib/python3.9/site-packages/cuspatial/core/spatial/join.py:171: UserWarning: scale 5 is less than required minimum scale 9345.561538461538. Clamping to minimum scale\n  warnings.warn(\n\n\nCPU times: user 649 ms, sys: 40.1 ms, total: 689 ms\nWall time: 686 ms\n\n\nAnd the same with geopandas:\n\n%%time\ntst = geopandas.sjoin(uprn, ss, how='left')\n\nCPU times: user 1.78 s, sys: 757 µs, total: 1.78 s\nWall time: 1.78 s\n\n\nWe can see computation time is much shorter on the GPU (this gap actually grows notably when the number of points grows, to obtain at least a 20x performance boost). To compare the two results, we join them into a single table:\n\ncheck = tst.join(\n    tst_gpu.to_pandas().set_index('LMK_KEY'), \n    on='LMK_KEY', \n    rsuffix='_gpu'\n)\n\nAnd check that the unique identifier of each EPC property (id and id_gpu) are the same:\n\n(check['id'] != check['id_gpu']).sum()\n\n1\n\n\nThe only instance in this sample that differs actually doesn’t differ but it is a point that is not joined to any polygon and hence has NaN values:\n\ncheck[check.eval('id != id_gpu')]\n\n\n\n\n\n  \n    \n      \n      LMK_KEY\n      CONSTRUCTION_AGE_BAND\n      UPRN\n      geometry\n      index_right\n      id\n      code\n      type\n      point_index\n      UPRN_gpu\n      CONSTRUCTION_AGE_BAND_gpu\n      id_gpu\n      type_gpu\n    \n  \n  \n    \n      559\n      887304392732013022216585817278109\n      England and Wales: 2007 onwards\n      10090070569\n      POINT (452546.000 533673.000)\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nWith this, we confirm we can use the GPU-backed spatial join, and proceed to deployment to the entire dataset.\n\n\n\nWe read in RAM the two tables without subsetting this time:\n\n%%time\nuprn = geopandas.read_parquet(uprn_p)\nss = geopandas.read_parquet(ss_p)\n\nCPU times: user 24.7 s, sys: 7.48 s, total: 32.1 s\nWall time: 28 s\n\n\nThen we move them to the GPU:\n\n%%time\nuprn_gpu = cuspatial.from_geopandas(uprn)\nss_gpu = cuspatial.from_geopandas(ss)\n\nCPU times: user 3min 46s, sys: 6.65 s, total: 3min 52s\nWall time: 3min 49s\n\n\nAnd we are ready to perform the GPU-backed spatial join. Because the GPU on which this is being run only has 8GB or memory, we need to chunk the computation. We will do this by joining chunk_size points at a time and storing the results back on RAM. Once finished, we save the resulting table to disk.\nWe can set this up with a simple for loop:\n\n%%time\nout = []\nchunk_size = 500000\nfor i in tqdm(range(ceil(len(uprn_gpu) / chunk_size))):\n    chunk = uprn_gpu.iloc[i*(chunk_size-1): i*(chunk_size-1)+chunk_size, :]\n    sjoined = sjoin_gpu(chunk, ss_gpu, scale=10000)\n    out.append(sjoined.to_pandas())\nout = pandas.concat(out)\nout.to_parquet('/rapids/notebooks/data/tmp/epc_uprn_ss.pq')\n\n 37%|███▋      | 17/46 [06:55<13:08, 27.19s/it]\n\n\n\n! du -h /rapids/notebooks/data/tmp/epc_uprn*\n\n\n\n\nWe replicate the approach above to join the centroid of each postcode to the spatial signature where they are located. For this, we first read into RAM both tables, postcode centroids and signature polygons:\n\n%%time\npc = geopandas.read_parquet(pc_p)\nss = geopandas.read_parquet(ss_p)\n\nCPU times: user 630 ms, sys: 211 ms, total: 841 ms\nWall time: 742 ms\n\n\nThen we move them to the GPU:\n\n%%time\npc_gpu = cuspatial.from_geopandas(pc)\nss_gpu = cuspatial.from_geopandas(ss)\n\nCPU times: user 11.1 s, sys: 516 ms, total: 11.6 s\nWall time: 11.6 s\n\n\nAnd we are ready to perform the GPU-backed spatial join. In this case, the dataset fits into the GPU all at once, so the code is greatly simplified:\n\n%%time\npc_ss = sjoin_gpu(\n    pc_gpu, ss_gpu, pts_cols=['PCD', 'lr_upc'], poly_cols=['id', 'type']\n)\n\n/opt/conda/envs/rapids/lib/python3.9/site-packages/cuspatial/core/spatial/indexing.py:193: UserWarning: scale 5 is less than required minimum scale 9345.561538461538. Clamping to minimum scale\n  warnings.warn(\n/opt/conda/envs/rapids/lib/python3.9/site-packages/cuspatial/core/spatial/join.py:171: UserWarning: scale 5 is less than required minimum scale 9345.561538461538. Clamping to minimum scale\n  warnings.warn(\n\n\nCPU times: user 1min 6s, sys: 104 ms, total: 1min 6s\nWall time: 1min 6s\n\n\nNow we can bring back the table we prepared earlier, attach signature types to each Land Registry sale, and write back to disk:\n\n(\n    pandas.read_parquet(\n        '/rapids/notebooks/data/tmp/sales_by_month_pc.pq'\n    )\n    .join(\n        pc_ss.to_pandas()[['lr_upc', 'id', 'type']].set_index('lr_upc'),\n        on='postcode'\n    )\n    .dropna()\n).to_parquet('/rapids/notebooks/data/tmp/sales_by_month_pc_ss.pq')\n\n\n\n\nSince the method used to perform the spatial join (sjoin_gpu) was written for this project, it might be helpful to print here its documentation:\n\n\nYou can download the file with the function here.\n\nsjoin_gpu?\n\n\u001b[0;31mSignature:\u001b[0m\n\u001b[0msjoin_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpts_gdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpoly_gdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpts_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LMK_KEY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UPRN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CONSTRUCTION_AGE_BAND'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpoly_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m\nSpatial Join on a GPU\n...\n\nAdapted from:\n\n> https://docs.rapids.ai/api/cuspatial/stable/user_guide/users.html#cuspatial.quadtree_point_in_polygon\n\nArguments\n---------\npts_gdf : geopandas.GeoDataFrame/cuspatial.GeoDataFrame\n          Table with points\npoly_gdf : geopandas.GeoDataFrame/cuspatial.GeoDataFrame\n           Table with polygons\nscale : int\n        [From `cuspatial` docs. Default=5] A scaling function that increases the size of the point \n        space from an origin defined by `{x_min, y_min}`. This can increase the likelihood of \n        generating well-separated quads.\n        \nmax_depth : int\n            [From `cuspatial` docs. Default=7] In order for a quadtree to index points effectively, \n            it must have a depth that is log-scaled with the size of the number of points. Each level \n            of the quad tree contains 4 quads. The number of available quads $q$\n            for indexing is then equal to $q = 4^d$ where $d$ is the max_depth parameter. With an input \n            size of 10m points and `max_depth` = 7, points will be most efficiently packed into the leaves\n            of the quad tree.\nmax_size : int\n           [From `cuspatial` docs. Default=125] Maximum number of points allowed in a node before it's \n           split into 4 leaf nodes. \npts_cols : list\n           [Optional. Default=['UPRN', 'CONSTRUCTION_AGE_BAND']] Column names in `pts_gdf` to be \n           joined in the output\npoly_cols : list\n            [Optional. Default=['id', 'type']] Column names in `poly_gdf` to be joined in the output \n\nReturns\n-------\nsjoined : cudf.DataFrame\n          Table with `pts_cols` and `poly_cols` spatially joined\n\u001b[0;31mFile:\u001b[0m      /rapids/notebooks/work/tools.py\n\u001b[0;31mType:\u001b[0m      function"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Age Capsule",
    "section": "",
    "text": "This is a capsule This resource is part of the Urban Grammar project and contains a capsule. Capsules are informally defined as bits of work that take “more than a blog post but less than anything else”. Capsules are explorations of how the data products generated by the Urban Grammar are useful in a variety of contexts. They are data-driven narratives, presented in as close to fully reproducible way as possible and, as the name suggest, encapsulated in their own repository so they are portable.\nIf you need to, you can cite this capsule using the following citation:\n@misc{arribasbel2022epc_capsule,\n  title        = \"Age Capsule\",\n  author       = \"{Dani Arribas-Bel}\",\n  howpublished = \"\\url{https://urbangrammarai.xyz/epc_capsule}\",\n  year         = 2022,\n  doi          = \"TBC\"\n}"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Age Capsule",
    "section": "",
    "text": "Signatures and EPC age\n\n\nHeatmap type x age\n\nSome form of time series\n\nSignatures and LR\n\n\nTime series of each class\n\nProportions over time\n\n\n\nimport pandas\nimport seaborn as sns\n\nepc_p = '/home/jovyan/data/tmp/epc_uprn_ss_simplified.pq'\npc_p = '/home/jovyan/data/tmp/postcode_ss.pq'\npct_p = '/home/jovyan/data/tmp/sales_by_month_pc_ss.pq'\n\ntype_order = [\n    'Hyper concentrated urbanity', \n    'Concentrated urbanity',\n    'Metropolitan urbanity', \n    'Regional urbanity',\n    'Local urbanity',\n    'Dense urban neighbourhoods',\n    'Dense residential neighbourhoods', \n    'Connected residential neighbourhoods', \n    'Gridded residential quarters',\n    'Accessible suburbia', \n    'Disconnected suburbia', \n    'Open sprawl', \n    'Warehouse/Park land', \n    'Urban buffer', \n    'Countryside agriculture',\n    'Wild countryside'\n]\n\n\n\nOur goal hear is to create a table where we have the number of properties listed in the EPC database by age (as recorded by the EPCs) and by spatial signature. To make this more tractable, we have manually created a mapping for all the entries in the age column to a grouping that makes easier to work with. We read the mapping and set the intended order first:\n\nage_remapping = (\n    pandas.read_csv('data/age_remapping.csv')\n    .set_index('src')\n    ['tgt']\n)\nage_order = [\n    'Before 1900', \n    '1900-1929', \n    '1930-1949',\n    '1950-1966', \n    '1967-1975', \n    '1976-1982', \n    '1983-1990', \n    '1991-1995',\n    '1996-2002', \n    '2003-2006', \n    '2007 onwards', \n]\n\nThen we read the table of all EPC properties and apply the mapping:\n\nepc = pandas.read_parquet(epc_p)\nepc['age_remapped'] = epc['CONSTRUCTION_AGE_BAND'].map(age_remapping)\n\nAt this point, we can create the table and write it to a .csv file:\n\nhm = epc.groupby(['age_remapped', 'type']).size().unstack()[type_order]\n\nhm.to_csv('data/epc_age_by_ss.csv')\n\n\n\nh = sns.heatmap(hm.reindex(age_order).T, cmap='viridis', cbar=False)\nh.set_xticklabels(h.get_xticklabels(), rotation = 45, ha=\"right\")\nh.set_title('No. of properties by Signature by Period');\n\n\n\n\n\ntab = hm.reindex(age_order).div(hm.T.sum(axis=1)).T\nh = sns.heatmap(tab, cmap='viridis', cbar=False)\nh.set_xticklabels(h.get_xticklabels(), rotation = 45, ha=\"right\")\nh.set_title('% of properties in Signature by period');\n\n\n\n\n\ntab = (hm.T / hm.T.sum())[age_order]\nh = sns.heatmap(tab, cmap='viridis', cbar=False)\nh.set_xticklabels(h.get_xticklabels(), rotation = 45, ha=\"right\")\nh.set_title('% of properties in period by Signature');\n\n\n\n\n\n\n\nOur goal here is to create a table that records all new sales by month (since the Land Registry database starts) by spatial signature. We create it from the table of all sales by postcode by month, and write it to a manageable .csv file:\n\nsales = (\n    pandas.read_parquet(pct_p)\n    .groupby(['moy', 'type'])\n    ['new_sales']\n    .sum()\n    .unstack()\n)\n\nsales.to_csv('data/lr_sales_by_month_ss.csv')\n\n\n\nsales.plot(figsize=(18, 19), subplots=True, sharex=True, sharey=False);\n\n\n\n\n\nsales.plot(figsize=(18, 19), subplots=True, sharex=True, sharey=True);"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Urban Grammar - Age Capsule",
    "section": "",
    "text": "TL;DR\nArribas-Bel and Fleischmann (2022) and Fleischmann and Arribas-Bel (2022).\n\n\nCode\nimport pandas\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport urbangrammar_graphics as ugg\n\ntype_order = [\n    'Hyper concentrated urbanity', \n    'Concentrated urbanity',\n    'Metropolitan urbanity', \n    'Regional urbanity',\n    'Local urbanity',\n    'Dense urban neighbourhoods',\n    'Dense residential neighbourhoods', \n    'Connected residential neighbourhoods', \n    'Gridded residential quarters',\n    'Accessible suburbia', \n    'Disconnected suburbia', \n    'Open sprawl', \n    'Warehouse/Park land', \n    'Urban buffer', \n    'Countryside agriculture',\n    'Wild countryside'\n][::-1]\n\nepc_age_order = age_order = [\n    'Before 1900', \n    '1900-1929', \n    '1930-1949',\n    '1950-1966', \n    '1967-1975', \n    '1976-1982', \n    '1983-1990', \n    '1991-1995',\n    '1996-2002', \n    '2003-2006', \n    '2007 onwards', \n]\n\nepc = pandas.read_csv(\n    'data/epc_age_by_ss.csv', \n    index_col='age_remapped',\n    skiprows=[-1]\n)[type_order[::-1]].reindex(epc_age_order)\nepc.index.name = ''\nlr = pandas.read_csv(\n    'data/lr_sales_by_month_ss.csv', index_col='moy'\n)\n\n\n\n\nIntroduction\n\n\nBuilding properties across signatures\n\n\nCode\nax = epc.sum(axis=0)[::-1].plot.barh(figsize=(4, 4), color=ugg.HEX[2])\nax.set_xticks([])\nax.tick_params(left=False)\nax.set_frame_on(False)\n\n\n\n\n\nProportion of EPC properties by signature type\n\n\n\n\n\n\nThe long view\n\n\nCode\nf, ax = plt.subplots(1, figsize=(9, 4))\nh = sns.heatmap(\n    epc.T, \n    cmap=sns.light_palette(ugg.HEX[4], as_cmap=True), \n    cbar=False, \n    linewidths=0.01, \n    linecolor='w',\n    ax=ax\n)\nh.tick_params(left=False, bottom=False)\nh.set_xticklabels(h.get_xticklabels(), rotation = 45, ha=\"right\");\n\n\n\n\n\nEPC properties by signature type over time periods\n\n\n\n\n\n\nCode\ntab = epc.reindex(age_order).div(epc.T.sum(axis=1)).T\n\nf, ax = plt.subplots(1, figsize=(7, 4))\nh = sns.heatmap(\n    tab, \n    cmap=sns.light_palette(ugg.HEX[4], as_cmap=True), \n    cbar=False, \n    linewidths=0.01, \n    linecolor='w',\n    ax=ax\n)\nh.tick_params(left=False, bottom=False)\nh.set_xticklabels(h.get_xticklabels(), rotation = 45, ha=\"right\");\n\n\n\n\n\nProportion of EPC properties by signature type over time periods (rows add up to 100%)\n\n\n\n\n\n\nCode\ntab = (epc.T / epc.T.sum())[age_order]\n\nf, ax = plt.subplots(1, figsize=(7, 4))\nh = sns.heatmap(\n    tab, \n    cmap=sns.light_palette(ugg.HEX[4], as_cmap=True), \n    cbar=False, \n    linewidths=0.01, \n    linecolor='w',\n    ax=ax\n)\nh.tick_params(left=False, bottom=False)\nh.set_xticklabels(h.get_xticklabels(), rotation = 45, ha=\"right\");\n\n\n\n\n\nProportion of EPC properties by signature type over time periods (columns add up to 100%)\n\n\n\n\n\n\nThe last 25 years\n\n\nCode\nax = lr.sum(axis=1).plot(\n    figsize=(2, 0.5), linewidth=0.5, color=ugg.HEX[0]\n)\nax.set_axis_off();\n\n\n\n\n\n\nLand Registry new properties\n\n\n\nThe overall timeline is available.\n\n\nCode\np = lr.plot(\n    figsize=(9, 4), subplots=False, sharex=True, sharey=True, alpha=0.5\n)\n\nhandles, labels = plt.gca().get_legend_handles_labels()\norder = pandas.Series(\n    range(len(labels)), index=labels\n)[type_order[::-1]].tolist()\nplt.legend(\n    [handles[idx] for idx in order],[labels[idx] for idx in order],\n    bbox_to_anchor=(1,1), \n    loc=\"upper left\",\n    frameon=False\n)\n\np.set_frame_on(False)\np.set_yticklabels([])\np.set_yticks([])\np.set_xlabel('')\np.tick_params(bottom=False)\np.set_xticklabels(p.get_xticklabels(), rotation = 45, ha=\"right\");\n\n\n\n\n\nLand Registry new properties by signature type\n\n\n\n\n\n\nCode\np = lr.T.div(lr.T.sum()).T.plot(\n    figsize=(9, 4), subplots=False, sharex=True, sharey=True, alpha=0.5\n)\n\nhandles, labels = plt.gca().get_legend_handles_labels()\norder = pandas.Series(\n    range(len(labels)), index=labels\n)[type_order[::-1]].tolist()\nplt.legend(\n    [handles[idx] for idx in order],[labels[idx] for idx in order],\n    bbox_to_anchor=(1,1), \n    loc=\"upper left\",\n    frameon=False\n)\n\np.set_frame_on(False)\np.set_yticklabels([])\np.set_yticks([])\np.set_xlabel('')\np.tick_params(bottom=False)\np.set_xticklabels(p.get_xticklabels(), rotation = 45, ha=\"right\");\n\n\n\n\n\nYearly proportion of Land Registry new properties by signature type\n\n\n\n\n\n\nCode\np = (\n    lr.T.div(lr.T.sum()).T\n    [['Open sprawl', 'Urban buffer']]\n    .multiply(100)\n    .plot.line(\n        figsize=(9, 4), color=(ugg.HEX[4], ugg.HEX[3])\n    )\n)\n\nplt.legend(loc=\"upper left\", frameon=False)\n\np.set_frame_on(False)\np.tick_params(left = False, bottom=False)\np.set_ylabel('% of new builds in signature')\np.set_xlabel('')\np.set_xticklabels(p.get_xticklabels(), rotation = 45, ha=\"right\");\n\n\n\n\n\nYearly proportion of Land Registry new properties\n\n\n\n\n\n\nCode\np = (\n    lr.T.div(lr.T.sum()).T\n    [[i for i in lr.columns if ('urbanity' in i.lower())]]\n    .multiply(100)\n    .plot.line(\n        figsize=(9, 4), color=ugg.HEX[:5], \n    )\n)\n\np.set_frame_on(False)\np.tick_params(left = False, bottom=False)\np.set_ylabel('% of new builds in signature')\np.set_xlabel('')\np.legend(frameon=False)\np.set_xticklabels(p.get_xticklabels(), rotation = 45, ha=\"right\");\n\n\n\n\n\nYearly proportion of Land Registry new properties\n\n\n\n\n\n\nCode\ntypes = (\n    [i for i in lr.columns if 'neighbourhoods' in i.lower()] + \n    ['Gridded residential quarters'] +\n    [i for i in lr.columns if 'suburbia' in i.lower()]\n)\nps = (\n    lr.T.div(lr.T.sum()).T\n    [types]\n    .multiply(100)\n    .iloc[:-1, :]\n    .plot.line(\n        figsize=(6, 4), color=ugg.HEX, subplots=True, rot=45\n    )\n)\n\nfor p in ps:\n\n    p.set_frame_on(False)\n    p.tick_params(left = False, bottom=False)\n    p.set_yticks([])\n    p.set_xlabel('')\n    p.legend(\n        loc=\"upper left\",\n        bbox_to_anchor=(1,1), \n        frameon=False\n    );\n\n\n\n\n\nYearly proportion of Land Registry new properties\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nArribas-Bel, Daniel, and Martin Fleischmann. 2022. “Spatial Signatures - Understanding (Urban) Spaces Through Form and Function.” Habitat International 128: 102641. https://doi.org/https://doi.org/10.1016/j.habitatint.2022.102641.\n\n\nFleischmann, Martin, and Daniel Arribas-Bel. 2022. “Geographical Characterisation of British Urban Form and Function Using the Spatial Signatures Framework.” Scientific Data 9 (1): 1–15."
  },
  {
    "objectID": "data_acquisition.html",
    "href": "data_acquisition.html",
    "title": "Age Capsule",
    "section": "",
    "text": "This document collates the three main datasets used int his capsule: the Energy Performance Certificates (EPC), the UPRN locations, and the Spatial Signature polygons. We first link (through a table join) building age, through EPC, with UPRN locations, and then we bring the Spatial Signatures. The two are subsequently joined on the GPU in a separate notebook. Each section details the origin of the data.\n\nimport pandas\nimport geopandas\nimport dask_geopandas\nfrom pyogrio import read_dataframe\nimport warnings # To turn disable some known ones below\n\nuprn_p = '/home/jovyan/data/uk_os_openuprn/osopenuprn_202210.gpkg'\nepc_p = '/home/jovyan/data/uk_epc_certificates/'\nss_p = '/home/jovyan/data/tmp/spatial_signatures_GB.gpkg'\npp_p = '/home/jovyan/data/tmp/pp-complete.csv'\npc_p = '/home/jovyan/data/tmp/postcodes.csv'\n\nERROR 1: PROJ: proj_create_from_database: Open of /opt/conda/share/proj failed\n\n\nSome of the computations will be run in parallel through Dask, so we set up a client for a local cluster with 16 workers (as many as threads in the machine where this is run):\n\nimport dask.dataframe as ddf\nfrom dask.distributed import LocalCluster, Client\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\")\n    client = Client(LocalCluster(n_workers=16))\n\n2022-12-26 23:23:47,774 - distributed.diskutils - INFO - Found stale lock file and directory '/home/jovyan/work/code/epc_capsule/dask-worker-space/worker-c4ai1jjk', purging\n2022-12-26 23:23:47,775 - distributed.diskutils - INFO - Found stale lock file and directory '/home/jovyan/work/code/epc_capsule/dask-worker-space/worker-yg_m_jqk', purging\n\n\n\n\nThese need to be downloaded manually from the official website (https://epc.opendatacommunities.org/). Once unzipped, it is a collection of .csv files that can be processed efficiently with Dask. Here we specify the computation lazily:\n\ndtypes = {\n   'CONSTRUCTION_AGE_BAND': 'str',\n   'UPRN': 'str',\n   'LMK_KEY': 'str'\n}\ncerts_all = ddf.read_csv(\n    f'{epc_p}*/certificates.csv', \n    dtype=dtypes,\n    usecols=dtypes\n)\n\nAnd execute it on the Dask cluster, local in this case, to load them in RAM (NOTE: this will take a significant amount of RAM on your machine). Note that we drop rows with N/A values in either of the three columns as we need observations with the three valid.\n\n%%time\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\")\n    certs = certs_all.dropna().compute()\n\nCPU times: user 12.4 s, sys: 3.1 s, total: 15.5 s\nWall time: 38.6 s\n\n\n\n\n\nUPRN coordinates are unique identifiers for property in Britain. We source them from the Ordnance Survey’s Open UPRN product (https://www.ordnancesurvey.co.uk/business-government/products/open-uprn), which also needs to be downloaded manually. We access the GPKG format which contains the geometries created for each point already.\nTo consume them, we load them up in RAM (NOTE - this will take a significant amount of memory on your machine):\n\n\nThe approach using pyogrio seems to beat a multi-core implementation with dask-geopandas, possibly because the latter relies on geopandas.read_file, even though it spreads the computation it across cores. In case of interest, here’s the code:\nuprn = dask_geopandas.read_file(\n    uprn_p, npartitions=16\n).compute()\n\n%%time\nuprn = read_dataframe(uprn_p, columns=['UPRN', 'geometry'])\nuprn['UPRN'] = uprn['UPRN'].astype(str) \n\nCPU times: user 56.1 s, sys: 8.79 s, total: 1min 4s\nWall time: 1min 10s\n\n\n\n\n\nWith both tables ready in memory, we merge them so that we attach point geometries to all the EPC certificate points through their UPRNs.\n\n%%time\ndb = geopandas.GeoDataFrame(\n    certs.merge(\n        uprn, left_on='UPRN', right_on='UPRN', how='left'\n    ), crs=uprn.crs\n)\n\nCPU times: user 40.9 s, sys: 3.58 s, total: 44.4 s\nWall time: 43.4 s\n\n\nAfter the merge, we write the table to disk so it can be loaded later on for the spatial join:\n\ndb.to_parquet('/home/jovyan/data/tmp/epc_uprn.pq')\n\n/tmp/ipykernel_3312797/3783868997.py:1: UserWarning: this is an initial implementation of Parquet/Feather file support and associated metadata.  This is tracking version 0.1.0 of the metadata specification at https://github.com/geopandas/geo-arrow-spec\n\nThis metadata specification does not yet make stability promises.  We do not yet recommend using this in a production setting unless you are able to rewrite your Parquet/Feather files.\n\nTo further ignore this warning, you can do: \nimport warnings; warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')\n  db.to_parquet('/home/jovyan/data/tmp/epc_uprn.pq')\n\n\n\n\n\nFor the Spatial Signature boundaries, we rely on the official open data product. This can be downloaded programmatically from its Figshare location. You can download it directly with:\n\n! rm -f $ss_p # Remove if exsisting\n! wget -O $ss_p https://figshare.com/ndownloader/files/30904861\n\n--2022-12-21 17:30:16--  https://figshare.com/ndownloader/files/30904861\nResolving figshare.com (figshare.com)... 54.194.88.49, 52.17.229.77, 2a05:d018:1f4:d003:376b:de5c:3a42:a610, ...\nConnecting to figshare.com (figshare.com)|54.194.88.49|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/30904861/spatial_signatures_GB.gpkg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20221221/eu-west-1/s3/aws4_request&X-Amz-Date=20221221T173017Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=6c7b771aaa9d3262e8c5d21388e81b74dd21b6d622d36a17bac818dc7fe6a71e [following]\n--2022-12-21 17:30:17--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/30904861/spatial_signatures_GB.gpkg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20221221/eu-west-1/s3/aws4_request&X-Amz-Date=20221221T173017Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=6c7b771aaa9d3262e8c5d21388e81b74dd21b6d622d36a17bac818dc7fe6a71e\nResolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.85.75, 52.218.100.203, 52.92.1.232, ...\nConnecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.85.75|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 909824000 (868M) [application/octet-stream]\nSaving to: ‘/home/jovyan/data/tmp/spatial_signatures_GB.gpkg’\n\n/home/jovyan/data/t 100%[===================>] 867.68M  70.4MB/s    in 12s     \n\n2022-12-21 17:30:29 (72.6 MB/s) - ‘/home/jovyan/data/tmp/spatial_signatures_GB.gpkg’ saved [909824000/909824000]\n\n\n\n\n%%time\nss = read_dataframe(ss_p)\n\nCPU times: user 1.46 s, sys: 794 ms, total: 2.26 s\nWall time: 2.24 s\n\n\nThis is very detailed, which makes things much slower to run, so we simplify first:\n\n%%time\nsss = ss.simplify(10)\n\nCPU times: user 1min 17s, sys: 1.04 s, total: 1min 19s\nWall time: 1min 10s\n\n\nNow we can write to disk a Parquet table with the simplified geometries for consumption later in the GPU:\n\nss.assign(geometry=sss).to_parquet('/home/jovyan/data/tmp/sss.pq')\n\n/tmp/ipykernel_3312797/4276883947.py:1: UserWarning: this is an initial implementation of Parquet/Feather file support and associated metadata.  This is tracking version 0.1.0 of the metadata specification at https://github.com/geopandas/geo-arrow-spec\n\nThis metadata specification does not yet make stability promises.  We do not yet recommend using this in a production setting unless you are able to rewrite your Parquet/Feather files.\n\nTo further ignore this warning, you can do: \nimport warnings; warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')\n  ss.assign(geometry=sss).to_parquet('/home/jovyan/data/tmp/sss.pq')\n\n\n\n\n\n\n! wget \\\n    http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-complete.csv \\\n    -O $pp_p\n\n--2022-12-23 11:48:14--  http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-complete.csv\nResolving prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com (prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com)... 52.218.118.28, 52.218.120.108, 52.218.120.212, ...\nConnecting to prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com (prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com)|52.218.118.28|:80... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: http://prod1.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-complete.csv [following]\n--2022-12-23 11:48:14--  http://prod1.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-complete.csv\nResolving prod1.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com (prod1.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com)... 52.218.120.108, 52.218.120.212, 52.92.19.172, ...\nReusing existing connection to prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com:80.\nHTTP request sent, awaiting response... 200 OK\nLength: 4849144007 (4.5G) [text/csv]\nSaving to: ‘/home/jovyan/data/tmp/pp-complete.csv’\n\n/home/jovyan/data/t 100%[===================>]   4.52G  30.3MB/s    in 1m 47s  \n\n2022-12-23 11:50:02 (43.1 MB/s) - ‘/home/jovyan/data/tmp/pp-complete.csv’ saved [4849144007/4849144007]\n\n\n\nFollowing the official documentation, the column names are:\n\ncol_names = [\n    'tid',\n    'price',\n    'date_of_transfer',\n    'postcode',\n    'property_type',\n    'new_build',\n    'duration',\n    'PAON',\n    'SAON',\n    'street',\n    'locality',\n    'town_city',\n    'district',\n    'county',\n    'ppd_cat_type',\n    'rec_status'\n]\n\nWe only read a subset of those:\n\n%%time\npp = ddf.read_csv(\n    pp_p, \n    names=col_names,\n    usecols=['tid', 'date_of_transfer', 'postcode', 'new_build'],\n    parse_dates=['date_of_transfer']\n).compute()\n\nCPU times: user 9.32 s, sys: 3.14 s, total: 12.5 s\nWall time: 21.9 s\n\n\nFor the analysis, we will need counts by month by postcode. We can calculate these already and save space:\n\n%%time\nsales = (\n    pp\n    .assign(moy=pp['date_of_transfer'].dt.to_period('M'))\n    .query('new_build == \"Y\"')\n    .groupby(['moy', 'postcode'])\n    .size()\n    .reset_index()\n    .rename(columns={0: 'new_sales'})\n)\n\nCPU times: user 6.26 s, sys: 769 ms, total: 7.03 s\nWall time: 6.86 s\n\n\nWe write the table as we will need it later on in the analysis, once it’s joined to the spatial signatures:\n\nsales.to_parquet('/home/jovyan/data/tmp/sales_by_month_pc.pq')\n\n\n\n\nPostcode locations (centroids) come from the ONSPD database:\n\n! wget \\\n    https://geoportal.statistics.gov.uk/datasets/2e65b9933cd9483b8724760f27968a48_0.csv \\\n    -O $pc_p\n\n--2022-12-23 14:41:43--  https://geoportal.statistics.gov.uk/datasets/2e65b9933cd9483b8724760f27968a48_0.csv\nResolving geoportal.statistics.gov.uk (geoportal.statistics.gov.uk)... 44.207.123.71, 3.219.120.199, 34.193.115.202\nConnecting to geoportal.statistics.gov.uk (geoportal.statistics.gov.uk)|44.207.123.71|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/csv]\nSaving to: ‘/home/jovyan/data/tmp/postcodes.csv’\n\n/home/jovyan/data/t     [   <=>              ]   1.10G  15.1MB/s    in 50s     \n\n2022-12-23 14:42:33 (22.6 MB/s) - ‘/home/jovyan/data/tmp/postcodes.csv’ saved [1186245382]\n\n\n\nWe read in parallel only the columns we need and drop rows with any missing value as we need postcodes for which we have the three features (i.e., IDs and location coordinates):\n\n%%time\npcs = ddf.read_csv(\n    pc_p,\n    usecols=['PCD', 'OSEAST1M', 'OSNRTH1M'],\n    assume_missing=True\n).dropna().compute()\n\nCPU times: user 535 ms, sys: 130 ms, total: 665 ms\nWall time: 1.83 s\n\n\nWe generate the point geometries:\n\nxys = geopandas.points_from_xy(\n    pcs['OSEAST1M'], pcs['OSNRTH1M']\n)\n\nWe can now build the geo-table with the point geometries of all available postcodes:\n\npc_pts = (\n    geopandas.GeoDataFrame(\n        pcs[['PCD']], geometry=xys\n    ).set_crs(epsg=27700)\n)\n\nThe ONSPD appears to contain postcodes expressed with a space in between and without:\n\npc_pts[pc_pts['PCD'].str.contains(' ')].head(1)\n\n\n\n\n\n  \n    \n      \n      PCD\n      geometry\n    \n  \n  \n    \n      0\n      AB1 0AA\n      POINT (385386.000 801193.000)\n    \n  \n\n\n\n\n\npc_pts[~pc_pts['PCD'].str.contains(' ')].head(1)\n\n\n\n\n\n  \n    \n      \n      PCD\n      geometry\n    \n  \n  \n    \n      2655\n      AB101AA\n      POINT (394251.000 806376.000)\n    \n  \n\n\n\n\nWhile the postcodes in the Land Registry all are expressed with a space (with the exception of an UNKNOWN instance):\n\nlr_upcs = pandas.Series(sales['postcode'].unique())\nlr_upcs[~lr_upcs.str.contains(' ')]\n\n201210    UNKNOWN\ndtype: object\n\n\nTo connect the two tables, we join them only after removing spaces in both sets of postcodes (which finds a geometry for the vast majority of postcodes):\n\nj = geopandas.GeoDataFrame(\n    pandas.DataFrame(\n        {'lr_upc': lr_upcs, 'jlr_upc': lr_upcs.str.replace(' ', '')}\n    )\n    .join(\n        pc_pts.assign(jPCD=pc_pts['PCD'].str.replace(' ', '')).set_index('jPCD'), \n        on='jlr_upc',\n        how='left'\n    )\n    .drop(columns=['jlr_upc'])\n).set_crs(pc_pts.crs)\n\nj.info()\n\n<class 'geopandas.geodataframe.GeoDataFrame'>\nRangeIndex: 284126 entries, 0 to 284125\nData columns (total 3 columns):\n #   Column    Non-Null Count   Dtype   \n---  ------    --------------   -----   \n 0   lr_upc    284126 non-null  object  \n 1   PCD       283932 non-null  object  \n 2   geometry  283932 non-null  geometry\ndtypes: geometry(1), object(2)\nmemory usage: 6.5+ MB\n\n\nWe write this to disk to be able to join it to spatial signature types on a GPU:\n\nj.to_parquet('/home/jovyan/data/tmp/postcode_pts.pq')\n\n/tmp/ipykernel_3543616/936528254.py:1: UserWarning: this is an initial implementation of Parquet/Feather file support and associated metadata.  This is tracking version 0.1.0 of the metadata specification at https://github.com/geopandas/geo-arrow-spec\n\nThis metadata specification does not yet make stability promises.  We do not yet recommend using this in a production setting unless you are able to rewrite your Parquet/Feather files.\n\nTo further ignore this warning, you can do: \nimport warnings; warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')\n  j.to_parquet('/home/jovyan/data/tmp/postcode_pts.pq')"
  }
]